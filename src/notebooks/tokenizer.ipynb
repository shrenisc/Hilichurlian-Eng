{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD\n",
    "from tokenizers.pre_tokenizers import Whitespace, Sequence, Digits, Punctuation\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\",))\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD()])\n",
    "tokenizer.pre_tokenizer = Sequence([Whitespace(), Digits(individual_digits=False), Punctuation()])\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[START] $A [END]\",\n",
    "    special_tokens=[\n",
    "        (\"[START]\", 0),\n",
    "        (\"[END]\", 1),\n",
    "    ],\n",
    ")\n",
    "trainer = WordPieceTrainer(vocab_size=1000, special_tokens=[\"[START]\", \"[END]\", \"[UNK]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], \n",
    "                     show_progress=True, initial_alphabet = list(string.ascii_letters) + list(string.digits) + list(string.punctuation))\n",
    "tokenizer.decoder = decoders.WordPiece()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        but that does n't mean that all , or many , or...\n",
      "1        there 's one called tor , which is another nut...\n",
      "2                             let us celebrate diversity .\n",
      "3        and all the evidence from around the world is ...\n",
      "4                                               whatever .\n",
      "                               ...                        \n",
      "10012    here you can see i 've added a light , i 'm tu...\n",
      "10013    i 'm sorry , you 've got to do a little bit of...\n",
      "10014                      he sees something in the path .\n",
      "10015    our modern world is communicating with itself ...\n",
      "10016                     let 's go through the evidence .\n",
      "Name: en, Length: 10017, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0        and as it rises with the strings , i see that ...\n",
      "1        but that does n't mean that all , or many , or...\n",
      "2        my travels to afghanistan began many , many ye...\n",
      "3        i see the polymerase and the enzymes and so fo...\n",
      "4                        you 've got to get out of there .\n",
      "                               ...                        \n",
      "61797    and you can be bent so much that you 're caugh...\n",
      "61798                     let 's go through the evidence .\n",
      "61799    tolerance is not really a lived virtue ; it 's...\n",
      "61800    `` so it 's a great way of representing quanti...\n",
      "61801    if we can get that into our soil bank , it 's ...\n",
      "Name: en, Length: 61802, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0        and when you improve searchability , you actua...\n",
      "1                             but what if it were active ?\n",
      "2                    but they did n't test for curiosity .\n",
      "3        and this conscious defiance is why i , as an a...\n",
      "4              you can use everything on the table on me .\n",
      "                               ...                        \n",
      "51780    that the crazy idea is just that , it is crazy...\n",
      "51781    now in both cases , i did n't send them home a...\n",
      "51782    in the dotted red line , we show what the adop...\n",
      "51783                  now , the caterpillar did n't die .\n",
      "51784    my students have problems : social , emotional...\n",
      "Name: en, Length: 51785, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0         retiring members nowadays say that it 's becom...\n",
      "1         no sophistication , no class , no dignity , no...\n",
      "2                            did you guess it ? companies .\n",
      "3         you ca n't travel very far or venture too far ...\n",
      "4                                           i was excited .\n",
      "                                ...                        \n",
      "182445    they do n't use the same infrastructure , and ...\n",
      "182446    and so we might even speak of the stream of co...\n",
      "182447                           he has 280,000 followers .\n",
      "182448    but we actually ran machine learning on youtub...\n",
      "182449    you 're mad . that 's an unfair offer , and yo...\n",
      "Name: en, Length: 182450, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD\n",
    "from tokenizers.pre_tokenizers import Whitespace, Sequence, Digits, Punctuation\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "lang=[[\"gl\",\"en\"],\n",
    "      [\"glpt\",\"en\"],\n",
    "      [\"pt\",\"en\"],\n",
    "      ['tr','en']]\n",
    "\n",
    "path=\"../../dataset/comparisions/\"\n",
    "\n",
    "for cur,target in lang:\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\",))\n",
    "    tokenizer.normalizer = normalizers.Sequence([NFD()])\n",
    "    tokenizer.pre_tokenizer = Sequence([Whitespace(), Digits(individual_digits=False), Punctuation()])\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[START] $A [END]\",\n",
    "        special_tokens=[\n",
    "            (\"[START]\", 0),\n",
    "            (\"[END]\", 1),\n",
    "        ],\n",
    "    )\n",
    "    trainer = WordPieceTrainer(vocab_size=1000, special_tokens=[\"[START]\", \"[END]\", \"[UNK]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], \n",
    "                        show_progress=True, initial_alphabet = list(string.ascii_letters) + list(string.digits) + list(string.punctuation))\n",
    "    tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "    name=path+cur+\"_to_\"+target+\"train.csv\"\n",
    "    data = pd.read_csv(name)\n",
    "    columns=list(data.columns)\n",
    "    X_train=data[columns[1]]\n",
    "    print(X_train)\n",
    "    def dataset_interator():\n",
    "        for article in X_train:\n",
    "            yield article\n",
    "    tokenizer.train_from_iterator(dataset_interator(), trainer=trainer)\n",
    "    tokenizer.save(f\"../../models/{cur}_tokeniser.json\")\n",
    "\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\",))\n",
    "    tokenizer.normalizer = normalizers.Sequence([NFD()])\n",
    "    tokenizer.pre_tokenizer = Sequence([Whitespace(), Digits(individual_digits=False), Punctuation()])\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[START] $A [END]\",\n",
    "        special_tokens=[\n",
    "            (\"[START]\", 0),\n",
    "            (\"[END]\", 1),\n",
    "        ],\n",
    "    )\n",
    "    trainer = WordPieceTrainer(vocab_size=1000, special_tokens=[\"[START]\", \"[END]\", \"[UNK]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], \n",
    "                        show_progress=True, initial_alphabet = list(string.ascii_letters) + list(string.digits) + list(string.punctuation))\n",
    "    tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "    name=path+cur+\"_to_\"+target+\"train.csv\"\n",
    "    data = pd.read_csv(name)\n",
    "    columns=list(data.columns)\n",
    "    X_train=data[columns[2]]\n",
    "    def dataset_interator():\n",
    "        for article in X_train:\n",
    "            yield article\n",
    "    tokenizer.train_from_iterator(dataset_interator(), trainer=trainer)\n",
    "    tokenizer.save(f\"../../models/{cur}s_{target}_tokeniser.json\")\n",
    "\n",
    "\n",
    "\n",
    "#X_train,X_test,y_train,y_test=train_test_split(data[\"Hilichurl\"],data[\"English\"],test_size=0.2,random_state=1234)\n",
    "#data = pd.read_csv(\"/home/rahulvadhyar/Documents/Hilichurlian-Eng/dataset/Hilichurl_Eng - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "    # for article in y_train:\n",
    "    #     yield article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(dataset_interator(), trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tokenizer.save(\"hilliTokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 126, 228, 165, 237, 246, 17, 126, 236, 246, 19, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mi Muhe Ye Beru Dada, Mi Valo Dada.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Mi Muhe Ye Beru Dada, Mi Valo Dada.\")\n",
    "print(output.ids)\n",
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 46, 101, 115, 82, 112, 17, 77, 102, 17, 46, 12, 82, 89, 111, 112, 85, 286, 116, 198, 109, 78, 101, 115, 82, 112, 19, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Its me, hi, I ' m the problem its me.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Its me, hi, I'm the problem its me.\")\n",
    "print(output.ids)\n",
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token odd-token\"  >I</span><span class=\"token even-token\"  >t</span><span class=\"token odd-token\"  >s</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >m</span><span class=\"token odd-token\"  >e</span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >h</span><span class=\"token even-token\"  >i</span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >I</span><span class=\"token odd-token\"  >'</span><span class=\"token even-token\"  >m</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >t</span><span class=\"token even-token\"  >h</span><span class=\"token odd-token\"  >e</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >p</span><span class=\"token odd-token\"  >ro</span><span class=\"token even-token\"  >b</span><span class=\"token odd-token\"  >le</span><span class=\"token even-token\"  >m</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >i</span><span class=\"token even-token\"  >t</span><span class=\"token odd-token\"  >s</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >m</span><span class=\"token odd-token\"  >e</span><span class=\"token even-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers.tools import EncodingVisualizer\n",
    "encoding = EncodingVisualizer(tokenizer)\n",
    "encoding(\"Its me, hi, I'm the problem its me.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
